\documentclass[11pt,onecolumn]{article}
\usepackage{amssymb, amsmath, amsthm,graphicx, paralist,algpseudocode,algorithm,cancel,url,color}
\usepackage{sectsty}
\usepackage{fancyvrb}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{booktabs}
\usepackage[table]{xcolor}
\usepackage{tikz}
% \usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}
\usepackage{listings}
\usepackage{enumitem}

\newcommand{\bvec}[1]{\mathbf{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Rn}{\R^{n\times n}}
\newcommand{\Rmn}{\R^{m\times n}}
\newcommand{\Cn}{\C^{n\times n}}
\newcommand{\Cmn}{\C^{m\times n}}
\newcommand{\cO}{\mathcal{O}}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\trace}{trace}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\vspan}{span}
\sectionfont{\Large\sc}
\subsectionfont{\sc}
\usepackage[margin=1 in]{geometry}
\begin{document}
\noindent
\textsc{\Large Numerical analysis: Homework 3}\\
Student: Pratyush Sudhakar (ps2245)\\
\hrule
\noindent

\subsection*{Question 1:}
Assume that we are given $A\in\mathbb{R}^{n\times n},$ $A=A^T,$ and $A$ has eigenvalue and vector pairs $\left\{(v_i,\lambda_i)\right\}_{i=1}^n.$ Furthermore, assume that $\lvert\lambda_1\rvert = \lvert\lambda_2\rvert > \lvert \lambda_3\rvert \geq \lvert\lambda_4\rvert \geq \cdots.$

\begin{enumerate}[label=(\alph*)]
    \item Prove that for any initial guess $v^{(0)}$ such that $v^{(0)}$ is not simultaneously orthogonal to both $v_1$ and $v_2$ the power method yields iterates $v^{(k)}$ that converge to lie in the span of $v_1$ and $v_2.$
          
          \textbf{Solution}:
          We know that $v_0$ is not orthogonal to both $v_1$ and $v_2$. Thus, we can write $$v_0 = c_1v_1 + c_2v_2 + \cdots + c_nv_n,$$ where not both $c_1$ and $c_2$ are zero.
          
          Now, by the power method, we have 
          \begin{align*}
              v^{(k)}          & = A^kv^{(0)}                                                          \\
                               & = A^k(c_1v_1 + c_2v_2 + \cdots + c_nv_n)                              \\
                               & = c_1\lambda_1^kv_1 + c_2\lambda_2^kv_2 + \cdots + c_n\lambda_n^kv_n. \\
              (\because A^kv_i & = \lambda_i^kv_i)                                              
          \end{align*}
          
          We can write this as,
          \begin{align*}
              v^{(k)}          & = c_1\lambda_1^k\left(v_1 + \frac{c_2}{c_1}\left(\frac{\lambda_2}{\lambda_1}\right)^kv_2 + \cdots + \frac{c_n}{c_1}\left(\frac{\lambda_n}{\lambda_1}\right)^kv_n\right);      \\
              \implies v^{(k)} & = c_1\lambda_1^k\left(v_1 + \frac{c_2}{c_1}\left(\frac{\lambda_2}{\lambda_1}\right)^kv_2 + \sum_{i=3}^{n}\frac{c_i}{c_1}\left(\frac{\lambda_i}{\lambda_1}\right)^kv_i\right).
          \end{align*}
          
          Since, $\lvert\lambda_1\rvert = \lvert\lambda_2\rvert > \lvert \lambda_3\rvert \geq \lvert\lambda_4\rvert \geq \cdots$, we have $\left(\frac{\lambda_2}{\lambda_1}\right) = \pm 1$ and $\left(\frac{\lambda_i}{\lambda_1}\right)^k \to 0$ as $k\to\infty$.
          Thus, we have
          \begin{align*}
              v^{(k)} & = c_1\lambda_1^k\left(v_1 + \frac{c_2}{c_1}\left(\frac{\lambda_2}{\lambda_1}\right)^kv_2 + \sum_{i=3}^{n}\frac{c_i}{c_1}\left(\frac{\lambda_i}{\lambda_1}\right)^kv_i\right) \\
                      & \approx c_1\lambda_1^k\left(v_1 \pm \frac{c_2}{c_1}v_2\right);\;\;          (\text{as $k\to \infty$})                                                                        \\
                      & = c_1\lambda_1^k\cdot v_1 \pm c_2\lambda_1^k\cdot v_2;                                                                                                
          \end{align*}
          where the $\pm$ sign depends on the sign of $\left(\frac{\lambda_2}{\lambda_1}\right)^k$. In both cases, we see that $v^{(k)}$ converges to the span of $v_1$ and $v_2$.
    \item What is the rate of convergence of $$\left(1-\left\|\left(v^{(k)}\right)^T\begin{bmatrix}v_1 & v_2\end{bmatrix}\right\|^2_2\right)^{1/2}?$$
          
          \textbf{Solution}:
          We know that $v^{(k)}$ converges to the span of $v_1$ and $v_2$. Thus, we can write $v^{(k)}$ as
          \begin{align*}
              v^{(k)} & = \alpha v_1 + \beta v_2 + \sum_{i=3}^{n}\gamma_iv_i,
          \end{align*}
          where $\alpha$ and $\beta$ are non-zero and $\gamma_i$ are such that $\sum_{i=3}^{n}\gamma_i^2 = 1$.
          
          Now, we have
          \begin{align*}
              \left(v^{(k)}\right)^T\begin{bmatrix}v_1 & v_2\end{bmatrix} & = \begin{bmatrix}\alpha & \beta\end{bmatrix}\begin{bmatrix}v_1^Tv_1 & v_1^Tv_2 \\ v_2^Tv_1 & v_2^Tv_2\end{bmatrix}  \\
                                                              & = \begin{bmatrix}\alpha & \beta\end{bmatrix}\begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix} \\
                                                              & = \begin{bmatrix}\alpha & \beta\end{bmatrix}.
          \end{align*}
          Thus, we have
          \begin{align*}
              \left\|\left(v^{(k)}\right)^T\begin{bmatrix}v_1 & v_2\end{bmatrix}\right\|^2_2 & = \left\|\begin{bmatrix}\alpha & \beta\end{bmatrix}\right\|^2_2 \\
                                                                                  & = \alpha^2 + \beta^2.
          \end{align*}
          Thus, we have
          \begin{align*}
              \left(1-\left\|\left(v^{(k)}\right)^T\begin{bmatrix}v_1 & v_2\end{bmatrix}\right\|^2_2\right)^{1/2} & = \left(1-\alpha^2 - \beta^2\right)^{1/2}.
          \end{align*}
          Thus, the rate of convergence of $\left(1-\left\|\left(v^{(k)}\right)^T\begin{bmatrix}v_1 & v_2\end{bmatrix}\right\|^2_2\right)^{1/2}$ is $\left(1-\alpha^2 - \beta^2\right)^{1/2}$.
          
    \item Does the associated eigenvalue estimate via the Rayleigh quotient necessarily converge in this setting? what about if $\lambda_1 = \lambda_2?$
\end{enumerate}

\subsection*{Question 2:}
Assume that we are given $A\in\mathbb{R}^{n\times n},$ $A=A^T,$ and $A$ has eigenvalue and vector pairs $\left\{(v_i,\lambda_i)\right\}_{i=1}^n.$ Furthermore, assume that $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_\ell > \lambda_{\ell+1}\geq \cdots \geq \lambda_n \geq 0.$ Now, say we run simultaneous iteration (also known an orthogonal iteration) to compute $\vspan\{v_1,\ldots,v_\ell\}$ and the associated eigenvalues $\lambda_1,\ldots,\lambda_\ell.$

\begin{enumerate}[label=(\alph*)]
    \item If we denote $\hat{\lambda}_1^{(k)}$ as our guess for $\lambda_1$ at iteration $k,$ show that $\lambda_1 \geq \hat{\lambda}_1^{(k)}$ for all $k.$ I.e., our guess for $\lambda_1$ converges from below.
    \item As we discussed in class, one reason to discuss convergence of the entire subspace is that it is insensitive to gaps (or the lack thereof) between the first $\ell$ eigenvalues. If we instead assume $\lambda_1 > \lambda_2 > \cdots > \lambda_\ell > \lambda_{\ell+1}\geq \cdots \geq \lambda_n \geq 0,$ would we expect the columns of $V^{(k)}$ (the ON basis for our guess at the invariant subspace of interest at iteration $k$) to converge to individual eigenvectors (in an appropriate sense)? If so, what might be expect the asymptotic rates of convergence to be? (For this last part a convincing argument suffices, we do not need a formal proof.)
\end{enumerate}

\subsection*{Question 3 (a more challenging, ungraded problem):}
Let $A$ be a $n\times n$ matrix that is not diagonalizable, and whose eigenvalue of largest magnitude, denoted $\lambda_1,$ is associated with a Jordan block of size two. You may assume the rest of the eigenvalues ($\lambda_2,\ldots,\lambda_{n-1}$) are simple. This means that there exists a matrix $X$ such that 
\[
    X^{-1}AX = \begin{bmatrix}\lambda_1 & 1 & \\ & \lambda_1 & \\ & & \Lambda \end{bmatrix}
\]
where $\Lambda$ is a diagonal matrix and $\|\Lambda\|_2<\lvert \lambda_1\rvert.$ Given essentially any initial guess, what, if anything, does the power method applied to $A$ converge to? If it does converge, at what rate does it do so? 

\end{document}